{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disseny del model DWTFormer\n",
    "\n",
    "Aquest notebook descriu la implementació de l'arquitectura híbrida DWTFormer, que combina la transformada wavelet discreta (DWT) amb capes Transformer per a la classificació d’imatges mèdiques en el context de prevenció de lesions esportives.\n",
    "\n",
    "L’objectiu és extreure característiques multiescala mitjançant DWT, i capturar relacions complexes entre regions d’una imatge amb el mecanisme d’atenció dels Transformers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definició del model DWTFormer en PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DWTFormer(nn.Module):\n",
    "    def __init__(self, patch_size=7, num_classes=9, d_model=64, nhead=4, num_layers=2):\n",
    "        super(DWTFormer, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (14 // patch_size) ** 2\n",
    "        self.flatten_dim = 4 * patch_size * patch_size  # 4 canals DWT\n",
    "\n",
    "        self.embedding = nn.Linear(self.flatten_dim, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, self.num_patches, d_model))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        patches = patches.contiguous().view(B, self.num_patches, -1)\n",
    "\n",
    "        x = self.embedding(patches) + self.positional_encoding\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paràmetres totals del model: 575,881\n",
      "Forma de sortida: torch.Size([2, 9])\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model = DWTFormer()\n",
    "print(f\"Paràmetres totals del model: {count_parameters(model):,}\")\n",
    "\n",
    "x = torch.randn(2, 4, 14, 14)\n",
    "out = model(x)\n",
    "print(\"Forma de sortida:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([2, 4, 14, 14])\n",
      "Patches: torch.Size([2, 4, 196])\n",
      "Embedding: torch.Size([2, 4, 64])\n",
      "Transformer Output: torch.Size([2, 4, 64])\n",
      "Output: torch.Size([2, 9])\n"
     ]
    }
   ],
   "source": [
    "def inspect_model(model, x):\n",
    "    with torch.no_grad():\n",
    "        print(\"Input:\", x.shape)\n",
    "        patches = x.unfold(2, 7, 7).unfold(3, 7, 7).contiguous().view(x.size(0), -1, 4*7*7)\n",
    "        print(\"Patches:\", patches.shape)\n",
    "        emb = model.embedding(patches)\n",
    "        print(\"Embedding:\", emb.shape)\n",
    "        trans = model.transformer(emb)\n",
    "        print(\"Transformer Output:\", trans.shape)\n",
    "        out = model.classifier(trans.mean(dim=1))\n",
    "        print(\"Output:\", out.shape)\n",
    "\n",
    "dummy_input = torch.randn(2, 4, 14, 14)\n",
    "inspect_model(model, dummy_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativa d’arquitectures\n",
    "\n",
    "| Model         | Preprocesament | Capacitat contextual | Cost computacional |\n",
    "|---------------|----------------|-----------------------|---------------------|\n",
    "| CNN bàsic     | Cap            | Local                 | Baix                |\n",
    "| VisionTransformer (ViT) | Patchify       | Global                | Alt                 |\n",
    "| **DWTFormer** | DWT + Patchify | Global + Multiescala  | Moderat             |\n",
    "\n",
    "La combinació DWT + Transformer permet capturar tant detalls locals (via subbandes wavelet) com relacions espacials globals (via atenció), essent un bon compromís entre eficiència i rendiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisions de disseny\n",
    "\n",
    "- **Patch size = 7**: divideix perfectament la imatge 14x14 en 4 blocs sense overlapping.\n",
    "- **4 canals d’entrada**: representen les subbandes LL, LH, HL i HH després de la DWT.\n",
    "- **Aggregació per mitjana**: simplifica la classificació global mantenint estabilitat.\n",
    "- **Positional Encoding**: permet mantenir relacions espacials entre els patches.\n",
    "\n",
    "Aquesta estructura modular facilita provar variants del model i escalar a imatges més grans.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
